{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Question Answering\n",
    "--------\n",
    "### NLP 243 - Machine Learning for Natural Language Processing\n",
    "\n",
    "Raghav Chaudhary, Adam Fidler, Sam Shamsan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "import stanza\n",
    "import inflect\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 61.2MB/s]\n",
      "2020-12-13 15:09:58 INFO: Downloading default packages for language: en (English)...\n",
      "2020-12-13 15:09:59 INFO: File exists: C:\\Users\\Raghav\\stanza_resources\\en\\default.zip.\n",
      "2020-12-13 15:10:03 INFO: Finished downloading models and saved to C:\\Users\\Raghav\\stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "# Download the necessary library files \n",
    "stanza.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-13 15:10:03 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| pos       | ewt       |\n",
      "| lemma     | ewt       |\n",
      "| depparse  | ewt       |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-12-13 15:10:03 INFO: Use device: gpu\n",
      "2020-12-13 15:10:03 INFO: Loading: tokenize\n",
      "2020-12-13 15:10:06 INFO: Loading: pos\n",
      "2020-12-13 15:10:07 INFO: Loading: lemma\n",
      "2020-12-13 15:10:07 INFO: Loading: depparse\n",
      "2020-12-13 15:10:08 INFO: Loading: sentiment\n",
      "2020-12-13 15:10:09 INFO: Loading: ner\n",
      "2020-12-13 15:10:10 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Pipline for NLP tasks such as parsing, tokenizing, etc.\n",
    "nlp = stanza.Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question-type Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model file for intent classification\n",
    "model = pickle.load(open('q_type_clf2.model','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based Dependency Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word inflections - turn \"people\" into \"person\", etc\n",
    "inflector = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_pos_tags(nlp_object):\n",
    "    return ([word.xpos for sent in nlp_object.sentences for word in sent.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_dep_tags(nlp_object):\n",
    "    return [word.deprel for sent in nlp_object.sentences for word in sent.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_tokens(nlp_object):\n",
    "    return [word.text for sent in nlp_object.sentences for word in sent.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_intention(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    nouns = []\n",
    "    pos = return_pos_tags(doc)\n",
    "    parse = return_dep_tags(doc)\n",
    "    text = return_tokens(doc)\n",
    "    for i in range(len(pos)):\n",
    "        if pos[i][:2] == 'NN':\n",
    "            nouns.append((text[i],parse[i]))\n",
    "    subj = inflector.singular_noun(nouns[0][0])\n",
    "    if not subj:\n",
    "        subj = nouns[0][0]\n",
    "    if len(nouns) > 1:\n",
    "        for i in range(len(nouns)):\n",
    "            if nouns[i][1] == 'nsubj':\n",
    "                subj = inflector.singular_noun(nouns[i][0])\n",
    "                if not subj:\n",
    "                    subj = nouns[i][0]\n",
    "    obj = [word for (word, tag) in nouns if word != subj]\n",
    "    return model.predict([sentence])[0], subj, obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Recognition CNN (YOLOv4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used a yolonet tutorial - https://www.learnopencv.com/deep-learning-based-object-detection-using-yolov3-with-opencv-python-c/\n",
    "\n",
    "confThreshold = 0.4  \n",
    "nmsThreshold = 0.4   \n",
    "inpWidth = 960       \n",
    "inpHeight = 960      \n",
    "classesFile = \"names.txt\"\n",
    "classes = None\n",
    "\n",
    "with open(classesFile, 'rt') as f:\n",
    "    classes = f.read().rstrip('\\n').split('\\n')\n",
    "modelConfiguration = \"yolo-obj.cfg\"\n",
    "modelWeights = \"yolov4.weights\" # Using pretrained weights on the MS-COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of the output layers\n",
    "def getOutputsNames(net):\n",
    "    # Get the names of all the layers in the network\n",
    "    layersNames = net.getLayerNames()\n",
    "    # Get the names of the output layers, i.e. the layers with unconnected outputs\n",
    "    return [layersNames[i[0] - 1] for i in net.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawPred(classId, conf, left, top, right, bottom):\n",
    "    # Draw a bounding box, but in jupyter notebooks this doesn't happen (Does with running from terminal)\n",
    "    cv.rectangle(frame, (left, top), (right, bottom), (255, 178, 50), 3)\n",
    "    \n",
    "    label = '%.2f' % conf\n",
    "\n",
    "    # Get the label for the class name and its confidence\n",
    "    if classes:\n",
    "        assert(classId < len(classes))\n",
    "        label = '%s:%s' % (classes[classId], label)\n",
    "    items.append(label)\n",
    "    \n",
    "    #Display the label at the top of the bounding box\n",
    "    labelSize, baseLine = cv.getTextSize(label, cv.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "    top = max(top, labelSize[1])\n",
    "    cv.rectangle(frame, (left, top - round(1.5*labelSize[1])), (left + round(1.5*labelSize[0]), top + baseLine), (255, 255, 255), cv.FILLED)\n",
    "    cv.putText(frame, label, (left, top), cv.FONT_HERSHEY_SIMPLEX, 0.75, (0,0,0), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(frame, outs):\n",
    "    frameHeight = frame.shape[0]\n",
    "    frameWidth = frame.shape[1]\n",
    "\n",
    "    # Scan through all the bounding boxes output from the network and keep only the\n",
    "    # ones with high confidence scores. Assign the box's class label as the class with the highest score.\n",
    "    classIds = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            classId = np.argmax(scores)\n",
    "            confidence = scores[classId]\n",
    "            if confidence > confThreshold:\n",
    "                center_x = int(detection[0] * frameWidth)\n",
    "                center_y = int(detection[1] * frameHeight)\n",
    "                width = int(detection[2] * frameWidth)\n",
    "                height = int(detection[3] * frameHeight)\n",
    "                left = int(center_x - width / 2)\n",
    "                top = int(center_y - height / 2)\n",
    "                classIds.append(classId)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([left, top, width, height])\n",
    "\n",
    "    # Perform non maximum suppression to eliminate redundant overlapping boxes with\n",
    "    # lower confidences.\n",
    "    indices = cv.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)\n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        box = boxes[i]\n",
    "        left = box[0]\n",
    "        top = box[1]\n",
    "        width = box[2]\n",
    "        height = box[3]\n",
    "        drawPred(classIds[i], confidences[i], left, top, left + width, top + height)\n",
    "        locations.append((left + width // 2, top + height // 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv.dnn.readNetFromDarknet(modelConfiguration, modelWeights)\n",
    "net.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv.dnn.DNN_TARGET_OPENCL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Info Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_information(image):\n",
    "    cap = cv.VideoCapture(image)\n",
    "    hasFrame, frame = cap.read()\n",
    "    blob = cv.dnn.blobFromImage(frame, 1/255, (inpWidth, inpHeight), [0,0,0], 1, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(getOutputsNames(net))\n",
    "    items = []\n",
    "    locations = []\n",
    "    postprocess(frame, outs)\n",
    "    #final_items = defaultdict(int)\n",
    "    final_items = defaultdict(lambda: {})\n",
    "\n",
    "    for a, b in zip(items, locations):\n",
    "        temp = a.split(\":\")[0]\n",
    "        if \"count\" not in final_items[temp]:\n",
    "            final_items[temp][\"count\"] = 0\n",
    "        final_items[temp][\"count\"] += 1\n",
    "        if \"location\" not in final_items[temp]:\n",
    "            final_items[temp][\"location\"] = []\n",
    "        final_items[temp][\"location\"].append(b)\n",
    "        final_items[temp]['location'].sort()\n",
    "    \n",
    "    return final_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for each final task\n",
    "\n",
    "# Uses the item dictionary to check for existence, and then returns a Y/N or a number depending on the flag\n",
    "def how_many(subj,d,is_there=False):\n",
    "    if 'count' not in d[subj]:\n",
    "        if is_there:\n",
    "            return 'No'\n",
    "        return 0\n",
    "    count = d[subj]['count']\n",
    "    if is_there and count:\n",
    "        return 'Yes'\n",
    "    elif is_there and not count:\n",
    "        return 'No'\n",
    "    return count\n",
    "\n",
    "def is_there(subj,d):\n",
    "    return how_many(subj,d,True)\n",
    "\n",
    "def what_is(d):\n",
    "    objects = [k for k in d.keys()]\n",
    "    return objects\n",
    "\n",
    "def where(subj,d):\n",
    "    locs = []\n",
    "    \n",
    "    for x,y in d[subj]['location']:\n",
    "   \n",
    "        loc = ''\n",
    "     \n",
    "        loc += \"Vertical: \"\n",
    "        if y <= 120:\n",
    "            loc += 'top'\n",
    "        elif y > 800:\n",
    "            loc += 'bottom'\n",
    "        else:\n",
    "            loc += 'middle'\n",
    "\n",
    "        loc += \", Horizontal:\"\n",
    "        if x <= 120:\n",
    "            loc += ' left'\n",
    "        elif x > 600:\n",
    "            loc += ' right'\n",
    "        else:\n",
    "            loc += ' middle'\n",
    "        locs.append(loc)\n",
    "    return locs\n",
    "\n",
    "def what_color(subj,d):\n",
    "    img = d[subj]['bbox']\n",
    "    def _closest_color(requested_color):\n",
    "        min_colors = {}\n",
    "        for key, name in webcolors.common_colors:\n",
    "            r_c, g_c, b_c = webcolors.hex_to_rgb(key)\n",
    "            rd = (r_c - requested_color[0]) ** 2\n",
    "            gd = (g_c - requested_color[1]) ** 2\n",
    "            bd = (b_c - requested_color[2]) ** 2\n",
    "            min_colors[(rd + gd + bd)] = name\n",
    "        return min_colors[min(min_colors.keys())]\n",
    "    def _get_color_name(requested_color):\n",
    "        try:\n",
    "            closest_name = actual_name = webcolors.rgb_to_name(requested_color)\n",
    "        except ValueError:\n",
    "            closest_name = _closest_color(requested_color)\n",
    "            actual_name = None\n",
    "        return closest_name\n",
    "    x,y,c = img.shape\n",
    "#     plt.imshow(img[x//3:2*x//3,y//3:2*y//3,:])\n",
    "    r,g,b = [int(np.mean(img[x//3:2*x//3,y//3:2*y//3,i])) for i in range(3)]\n",
    "    return _get_color_name((r,g,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary which maps intents to functions\n",
    "\n",
    "intent_to_functions = {}\n",
    "intent_to_functions['how many'] = how_many \n",
    "intent_to_functions['is this'] = is_there\n",
    "intent_to_functions['what color'] = what_color\n",
    "intent_to_functions['where'] = where\n",
    "intent_to_functions['other'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question, image_info):\n",
    "    outs = extract_intention(question)\n",
    "    query, subject, _ = outs\n",
    "    return(intent_to_functions[query](subject, image_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_info_dict = extract_image_information(\"veggies.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](veggies.jpg \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question(\"Is there brocolli in this image?\", image_info_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
